{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text-to-Image AI Service in Google Colab\n",
    "\n",
    "This notebook sets up and runs a text-to-image generation service using Stable Diffusion with TensorFlow and KerasCV.\n",
    "\n",
    "## Requirements\n",
    "- Enable GPU runtime: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n",
    "- High-RAM runtime recommended for better performance\n",
    "\n",
    "## Features\n",
    "- üé® Stable Diffusion image generation\n",
    "- üöÄ FastAPI REST API\n",
    "- üåê Public URL via ngrok\n",
    "- üì± Interactive web interface"
   ],
   "metadata": {
    "id": "title_cell"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Check GPU and Setup Environment"
   ],
   "metadata": {
    "id": "step1_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Set up environment variables\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['COLAB_GPU'] = '1'\n",
    "\n",
    "print(\"‚úÖ Environment check complete\")"
   ],
   "metadata": {
    "id": "check_gpu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Install Dependencies"
   ],
   "metadata": {
    "id": "step2_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev libgomp1\n",
    "\n",
    "print(\"‚úÖ System dependencies installed\")"
   ],
   "metadata": {
    "id": "install_system"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Install Python dependencies\n",
    "!pip install -q tensorflow>=2.15.0\n",
    "!pip install -q keras-cv>=0.6.0\n",
    "!pip install -q fastapi>=0.104.0\n",
    "!pip install -q uvicorn[standard]>=0.24.0\n",
    "!pip install -q python-multipart>=0.0.6\n",
    "!pip install -q pillow>=10.0.0\n",
    "!pip install -q pydantic>=2.5.0\n",
    "!pip install -q pydantic-settings>=2.1.0\n",
    "!pip install -q python-dotenv>=1.0.0\n",
    "!pip install -q structlog>=23.2.0\n",
    "!pip install -q nest-asyncio\n",
    "!pip install -q pyngrok\n",
    "\n",
    "print(\"‚úÖ Python dependencies installed\")"
   ],
   "metadata": {
    "id": "install_python"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Setup ngrok Authentication (REQUIRED)\n\n**IMPORTANT**: ngrok now requires authentication even for free usage.\n\n1. **Sign up for free**: Go to https://dashboard.ngrok.com/signup\n2. **Get your auth token**: Visit https://dashboard.ngrok.com/get-started/your-authtoken  \n3. **Copy the token**: It looks like `2abc123_def456ghi789jkl...`\n4. **Replace the token below**: Change `YOUR_NGROK_AUTH_TOKEN_HERE` to your actual token",
   "metadata": {
    "id": "step3_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Set up ngrok authentication (REQUIRED)\n# Get your auth token from: https://dashboard.ngrok.com/get-started/your-authtoken\n\nfrom pyngrok import ngrok, conf\n\n# REPLACE THIS with your actual ngrok auth token:\nNGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN_HERE\"\n\nif NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTH_TOKEN_HERE\":\n    print(\"‚ùå ERROR: Please set your ngrok auth token!\")\n    print(\"1. Go to: https://dashboard.ngrok.com/signup (free signup)\")\n    print(\"2. Get token: https://dashboard.ngrok.com/get-started/your-authtoken\")\n    print(\"3. Replace YOUR_NGROK_AUTH_TOKEN_HERE above with your token\")\n    raise ValueError(\"ngrok auth token required\")\nelse:\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    print(\"‚úÖ ngrok authentication configured successfully!\")",
   "metadata": {
    "id": "setup_ngrok"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Create and Run the Text-to-Image Service"
   ],
   "metadata": {
    "id": "step4_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import Optional, List\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "# Enable nested asyncio for Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ],
   "metadata": {
    "id": "imports"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Configure TensorFlow for Colab\n",
    "def setup_tensorflow():\n",
    "    \"\"\"Configure TensorFlow for Colab environment.\"\"\"\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "            print(f\"‚úÖ Configured {len(gpus)} GPU(s) with mixed precision\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"‚ö†Ô∏è GPU setup error: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU detected, using CPU (will be slow)\")\n",
    "    return len(gpus) > 0\n",
    "\n",
    "gpu_available = setup_tensorflow()"
   ],
   "metadata": {
    "id": "setup_tf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define API models\n",
    "class ImageRequest(BaseModel):\n",
    "    prompt: str = Field(..., min_length=1, max_length=500, description=\"Text description of the image\")\n",
    "    num_steps: int = Field(25, ge=10, le=50, description=\"Number of diffusion steps (lower = faster)\")\n",
    "    guidance_scale: float = Field(7.5, ge=1.0, le=15.0, description=\"How closely to follow the prompt\")\n",
    "    seed: Optional[int] = Field(None, ge=0, description=\"Random seed for reproducible results\")\n",
    "\n",
    "class ImageResponse(BaseModel):\n",
    "    image_base64: str\n",
    "    prompt: str\n",
    "    generation_time: float\n",
    "    parameters: dict\n",
    "\n",
    "print(\"‚úÖ API models defined\")"
   ],
   "metadata": {
    "id": "api_models"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create the Stable Diffusion model class\nclass ColabStableDiffusion:\n    \"\"\"Simplified Stable Diffusion model for Colab.\"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.load_model()\n\n    def load_model(self):\n        \"\"\"Load Stable Diffusion model.\"\"\"\n        print(\"üîÑ Loading Stable Diffusion model (this may take a few minutes)...\")\n\n        try:\n            self.model = keras_cv.models.StableDiffusion(\n                img_width=512,\n                img_height=512,\n                jit_compile=False,  # Disable for Colab compatibility\n            )\n            print(\"‚úÖ Stable Diffusion model loaded successfully!\")\n        except Exception as e:\n            print(f\"‚ùå Model loading failed: {e}\")\n            raise\n\n    def generate_image(self, prompt: str, num_steps: int = 25,\n                      guidance_scale: float = 7.5, seed: Optional[int] = None):\n        \"\"\"Generate image from text prompt.\"\"\"\n        if self.model is None:\n            raise RuntimeError(\"Model not loaded\")\n\n        if seed is not None:\n            tf.random.set_seed(seed)\n            np.random.seed(seed)\n\n        try:\n            print(f\"üé® Generating image for: {prompt[:50]}...\")\n            start_time = time.time()\n\n            # Check KerasCV version and use appropriate parameters\n            try:\n                # Try with guidance_scale first (newer versions)\n                generated_images = self.model.text_to_image(\n                    prompt=prompt,\n                    batch_size=1,\n                    num_steps=num_steps,\n                    guidance_scale=guidance_scale,\n                )\n            except TypeError as e:\n                if \"guidance_scale\" in str(e):\n                    print(\"‚ö†Ô∏è Using unconditional_guidance_scale parameter for older KerasCV version\")\n                    # Fallback for older versions that use unconditional_guidance_scale\n                    try:\n                        generated_images = self.model.text_to_image(\n                            prompt=prompt,\n                            batch_size=1,\n                            num_steps=num_steps,\n                            unconditional_guidance_scale=guidance_scale,\n                        )\n                    except TypeError:\n                        # If still failing, try without guidance parameter\n                        print(\"‚ö†Ô∏è Using basic parameters without guidance scale\")\n                        generated_images = self.model.text_to_image(\n                            prompt=prompt,\n                            batch_size=1,\n                            num_steps=num_steps,\n                        )\n                else:\n                    raise e\n\n            # Convert to PIL Image\n            img_array = generated_images[0]\n            img_array = (img_array + 1.0) * 127.5\n            img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n            pil_image = Image.fromarray(img_array)\n\n            generation_time = time.time() - start_time\n            print(f\"‚úÖ Image generated in {generation_time:.1f} seconds\")\n\n            return pil_image\n\n        except Exception as e:\n            print(f\"‚ùå Generation failed: {e}\")\n            raise\n\n# Initialize the model (this will take a few minutes)\nprint(\"Initializing Stable Diffusion model...\")\nsd_model = ColabStableDiffusion()",
   "metadata": {
    "id": "model_class"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create FastAPI application\n",
    "app = FastAPI(\n",
    "    title=\"Text-to-Image AI (Colab)\",\n",
    "    description=\"Stable Diffusion text-to-image generation running in Google Colab\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "@app.post(\"/generate\", response_model=ImageResponse)\n",
    "async def generate_image_endpoint(request: ImageRequest):\n",
    "    \"\"\"Generate image from text prompt.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Run generation in thread pool to avoid blocking\n",
    "        loop = asyncio.get_event_loop()\n",
    "        pil_image = await loop.run_in_executor(\n",
    "            None,\n",
    "            sd_model.generate_image,\n",
    "            request.prompt,\n",
    "            request.num_steps,\n",
    "            request.guidance_scale,\n",
    "            request.seed,\n",
    "        )\n",
    "\n",
    "        # Convert to base64\n",
    "        buffer = io.BytesIO()\n",
    "        pil_image.save(buffer, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        return ImageResponse(\n",
    "            image_base64=img_base64,\n",
    "            prompt=request.prompt,\n",
    "            generation_time=generation_time,\n",
    "            parameters={\n",
    "                \"num_steps\": request.num_steps,\n",
    "                \"guidance_scale\": request.guidance_scale,\n",
    "                \"seed\": request.seed\n",
    "            }\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": sd_model.model is not None,\n",
    "        \"gpu_available\": gpu_available,\n",
    "        \"environment\": \"Google Colab\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Root endpoint with usage instructions.\"\"\"\n",
    "    return {\n",
    "        \"message\": \"Text-to-Image AI Service running in Google Colab\",\n",
    "        \"docs_url\": \"/docs\",\n",
    "        \"health_url\": \"/health\",\n",
    "        \"generate_url\": \"/generate\",\n",
    "        \"gpu_available\": gpu_available\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ FastAPI application created\")"
   ],
   "metadata": {
    "id": "fastapi_app"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Start the Server with Public URL"
   ],
   "metadata": {
    "id": "step5_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Start the server with ngrok tunnel\nimport threading\nimport time\nimport asyncio\nfrom uvicorn import Config, Server\n\ndef start_server():\n    \"\"\"Start the FastAPI server with proper async handling.\"\"\"\n    print(\"üîÑ Starting FastAPI server...\")\n    try:\n        # Create a new event loop for this thread\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        \n        # Configure and start the server\n        config = Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n        server = Server(config)\n        \n        # Run the server\n        loop.run_until_complete(server.serve())\n    except Exception as e:\n        print(f\"‚ùå Server startup failed: {e}\")\n\n# Start the server in a separate thread BEFORE creating ngrok tunnel\nprint(\"üöÄ Starting the FastAPI server...\")\nserver_thread = threading.Thread(target=start_server)\nserver_thread.daemon = True\nserver_thread.start()\n\n# Wait for server to start\nprint(\"‚è≥ Waiting for server to initialize...\")\ntime.sleep(15)  # Increased wait time\n\n# Test if server is running locally\nimport requests\nserver_ready = False\nfor attempt in range(5):  # Try 5 times\n    try:\n        test_response = requests.get(\"http://localhost:8000/health\", timeout=5)\n        if test_response.status_code == 200:\n            print(\"‚úÖ Server is running locally!\")\n            server_ready = True\n            break\n        else:\n            print(f\"‚ö†Ô∏è Server responded with status: {test_response.status_code}\")\n    except Exception as e:\n        print(f\"üîÑ Attempt {attempt + 1}/5: Server not ready yet...\")\n        time.sleep(5)\n\nif not server_ready:\n    print(\"‚ùå Server failed to start properly. Please restart runtime and try again.\")\nelse:\n    # Now start ngrok tunnel (uses the auth token set earlier)\n    print(\"üåê Creating ngrok tunnel...\")\n    try:\n        public_tunnel = ngrok.connect(8000)\n        public_url = str(public_tunnel)  # Convert to string URL\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"üöÄ TEXT-TO-IMAGE AI SERVICE IS LIVE!\")\n        print(\"=\"*60)\n        print(f\"üåê Public URL: {public_url}\")\n        print(f\"üìñ API Documentation: {public_url}/docs\")\n        print(f\"‚ù§Ô∏è Health Check: {public_url}/health\")\n        print(f\"üé® Generate Images: {public_url}/generate\")\n        print(\"=\"*60)\n        print(\"\\nüì± How to use:\")\n        print(\"1. Click the API Documentation link above\")\n        print(\"2. Try the /generate endpoint with a text prompt\")\n        print(\"3. Or use curl/Python requests to call the API\")\n        print(\"\\n‚ö†Ô∏è Note: Keep this cell running to maintain the service\")\n        print(\"=\"*60)\n        \n    except Exception as e:\n        print(f\"‚ùå Failed to create ngrok tunnel: {e}\")\n\nif server_ready:\n    print(\"\\n‚úÖ Setup complete! The service should now be accessible via the public URL above.\")\nelse:\n    print(\"\\n‚ùå Setup failed. Please restart runtime and try again.\")",
   "metadata": {
    "id": "start_server"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 6: Troubleshooting & Server Check\n\nIf you're getting ngrok connection errors, run this cell to diagnose the issue:",
   "metadata": {
    "id": "step6_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Troubleshooting: Check if server is running properly\nimport requests\nimport time\nimport subprocess\nimport threading\nimport asyncio\nfrom uvicorn import Config, Server\n\nprint(\"üîç Diagnosing server status...\")\n\n# Check if port 8000 is in use\ntry:\n    result = subprocess.run(['netstat', '-tuln'], capture_output=True, text=True)\n    if ':8000' in result.stdout:\n        print(\"‚úÖ Port 8000 is in use (server might be running)\")\n    else:\n        print(\"‚ùå Port 8000 is not in use (server not running)\")\nexcept:\n    print(\"‚ö†Ô∏è Could not check port status\")\n\n# Test local server connection\nprint(\"\\nüîç Testing local server connection...\")\ntry:\n    response = requests.get(\"http://localhost:8000/health\", timeout=10)\n    print(f\"‚úÖ Local server is responding! Status: {response.status_code}\")\n    print(f\"Response: {response.json()}\")\nexcept requests.exceptions.ConnectionError:\n    print(\"‚ùå Cannot connect to local server on port 8000\")\n    print(\"üîÑ Trying to restart the server...\")\n    \n    # Try to restart server with proper async handling\n    def restart_server():\n        try:\n            # Create a new event loop for this thread\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            \n            # Configure and start the server\n            config = Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n            server = Server(config)\n            \n            # Run the server\n            loop.run_until_complete(server.serve())\n        except Exception as e:\n            print(f\"‚ùå Server restart failed: {e}\")\n    \n    server_thread = threading.Thread(target=restart_server)\n    server_thread.daemon = True\n    server_thread.start()\n    \n    print(\"‚è≥ Waiting 20 seconds for server restart...\")\n    time.sleep(20)\n    \n    # Test again\n    for attempt in range(3):\n        try:\n            response = requests.get(\"http://localhost:8000/health\", timeout=5)\n            print(f\"‚úÖ Server restarted successfully! Status: {response.status_code}\")\n            break\n        except:\n            print(f\"üîÑ Restart attempt {attempt + 1}/3...\")\n            time.sleep(5)\n    else:\n        print(\"‚ùå Server restart failed\")\n\nexcept Exception as e:\n    print(f\"‚ùå Error testing server: {e}\")\n\n# Test ngrok tunnel if it exists\nif 'public_url' in globals():\n    print(f\"\\nüîç Testing ngrok tunnel: {public_url}\")\n    try:\n        response = requests.get(f\"{public_url}/health\", timeout=30)\n        print(f\"‚úÖ Ngrok tunnel is working! Status: {response.status_code}\")\n    except Exception as e:\n        print(f\"‚ùå Ngrok tunnel test failed: {e}\")\n        print(\"üí° Try running the server setup cell again\")\nelse:\n    print(\"\\n‚ö†Ô∏è No ngrok tunnel found. Run the server setup cell first.\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"üí° TROUBLESHOOTING TIPS:\")\nprint(\"=\"*50)\nprint(\"1. If local server fails: Restart runtime and run all cells again\")\nprint(\"2. If ngrok fails: Check your auth token is set correctly\")  \nprint(\"3. If still failing: Try running cells one by one with delays\")\nprint(\"4. For memory issues: Enable High-RAM runtime\")\nprint(\"5. For async errors: Make sure nest_asyncio is installed\")\nprint(\"=\"*50)",
   "metadata": {
    "id": "test_api"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Test the API (Optional)",
   "metadata": {
    "id": "step7_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Test the API directly from the notebook\nimport requests\nimport json\nfrom IPython.display import Image as IPImage\nfrom io import BytesIO\n\ndef test_generation(prompt, num_steps=25):\n    \"\"\"Test image generation directly.\"\"\"\n    print(f\"Testing generation with prompt: {prompt}\")\n    \n    # Check if we have the public URL\n    if 'public_url' not in globals():\n        print(\"‚ùå Error: No public URL found. Run the server setup cell first.\")\n        return None\n    \n    # Ensure we have a proper URL string\n    test_url = public_url if isinstance(public_url, str) else str(public_url)\n    if not test_url.startswith(('http://', 'https://')):\n        print(\"‚ùå Error: Invalid URL format\")\n        return None\n\n    try:\n        print(f\"üåê Making request to: {test_url}\")\n        \n        # Make API request\n        response = requests.post(\n            f\"{test_url}/generate\",\n            json={\n                \"prompt\": prompt,\n                \"num_steps\": num_steps,\n                \"guidance_scale\": 7.5\n            },\n            timeout=300  # 5 minute timeout for generation\n        )\n\n        if response.status_code == 200:\n            result = response.json()\n            print(f\"‚úÖ Generation completed in {result['generation_time']:.1f} seconds\")\n\n            # Decode and display image\n            img_data = base64.b64decode(result['image_base64'])\n            return IPImage(img_data)\n        else:\n            print(f\"‚ùå Error: {response.status_code} - {response.text}\")\n            return None\n            \n    except requests.exceptions.RequestException as e:\n        print(f\"‚ùå Request failed: {e}\")\n        return None\n    except Exception as e:\n        print(f\"‚ùå Unexpected error: {e}\")\n        return None\n\n# Wait a moment for everything to be ready\nprint(\"‚è≥ Waiting for services to be ready...\")\ntime.sleep(5)\n\n# Test with a simple prompt\nprint(\"üé® Testing image generation...\")\ntest_image = test_generation(\"a cute cat sitting in a garden\", num_steps=20)\nif test_image:\n    display(test_image)\n    print(\"üéâ Success! Your text-to-image service is working!\")\nelse:\n    print(\"üîÑ If the test failed, try running the troubleshooting cell above.\")\n    print(\"üí° Common solutions:\")\n    print(\"   - Wait a bit longer (model might still be loading)\")\n    print(\"   - Run the troubleshooting cell to check server status\")\n    print(\"   - Restart runtime and run all cells again\")",
   "metadata": {
    "id": "keep_running"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Keep the Service Running",
   "metadata": {
    "id": "usage_examples"
   }
  }
 ]
}