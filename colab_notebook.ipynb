{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text-to-Image AI Service in Google Colab\n",
    "\n",
    "This notebook sets up and runs a text-to-image generation service using Stable Diffusion with TensorFlow and KerasCV.\n",
    "\n",
    "## Requirements\n",
    "- Enable GPU runtime: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n",
    "- High-RAM runtime recommended for better performance\n",
    "\n",
    "## Features\n",
    "- üé® Stable Diffusion image generation\n",
    "- üöÄ FastAPI REST API\n",
    "- üåê Public URL via ngrok\n",
    "- üì± Interactive web interface"
   ],
   "metadata": {
    "id": "title_cell"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Check GPU and Setup Environment"
   ],
   "metadata": {
    "id": "step1_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Set up environment variables\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['COLAB_GPU'] = '1'\n",
    "\n",
    "print(\"‚úÖ Environment check complete\")"
   ],
   "metadata": {
    "id": "check_gpu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Install Dependencies"
   ],
   "metadata": {
    "id": "step2_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev libgomp1\n",
    "\n",
    "print(\"‚úÖ System dependencies installed\")"
   ],
   "metadata": {
    "id": "install_system"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Install Python dependencies\n",
    "!pip install -q tensorflow>=2.15.0\n",
    "!pip install -q keras-cv>=0.6.0\n",
    "!pip install -q fastapi>=0.104.0\n",
    "!pip install -q uvicorn[standard]>=0.24.0\n",
    "!pip install -q python-multipart>=0.0.6\n",
    "!pip install -q pillow>=10.0.0\n",
    "!pip install -q pydantic>=2.5.0\n",
    "!pip install -q pydantic-settings>=2.1.0\n",
    "!pip install -q python-dotenv>=1.0.0\n",
    "!pip install -q structlog>=23.2.0\n",
    "!pip install -q nest-asyncio\n",
    "!pip install -q pyngrok\n",
    "\n",
    "print(\"‚úÖ Python dependencies installed\")"
   ],
   "metadata": {
    "id": "install_python"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Setup ngrok Authentication (REQUIRED)\n\n**IMPORTANT**: ngrok now requires authentication even for free usage.\n\n1. **Sign up for free**: Go to https://dashboard.ngrok.com/signup\n2. **Get your auth token**: Visit https://dashboard.ngrok.com/get-started/your-authtoken  \n3. **Copy the token**: It looks like `2abc123_def456ghi789jkl...`\n4. **Replace the token below**: Change `YOUR_NGROK_AUTH_TOKEN_HERE` to your actual token",
   "metadata": {
    "id": "step3_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Set up ngrok authentication (REQUIRED)\n# Get your auth token from: https://dashboard.ngrok.com/get-started/your-authtoken\n\nfrom pyngrok import ngrok, conf\n\n# REPLACE THIS with your actual ngrok auth token:\nNGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN_HERE\"\n\nif NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTH_TOKEN_HERE\":\n    print(\"‚ùå ERROR: Please set your ngrok auth token!\")\n    print(\"1. Go to: https://dashboard.ngrok.com/signup (free signup)\")\n    print(\"2. Get token: https://dashboard.ngrok.com/get-started/your-authtoken\")\n    print(\"3. Replace YOUR_NGROK_AUTH_TOKEN_HERE above with your token\")\n    raise ValueError(\"ngrok auth token required\")\nelse:\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    print(\"‚úÖ ngrok authentication configured successfully!\")",
   "metadata": {
    "id": "setup_ngrok"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Create and Run the Text-to-Image Service"
   ],
   "metadata": {
    "id": "step4_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import Optional, List\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "# Enable nested asyncio for Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ],
   "metadata": {
    "id": "imports"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Configure TensorFlow for Colab\n",
    "def setup_tensorflow():\n",
    "    \"\"\"Configure TensorFlow for Colab environment.\"\"\"\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "            print(f\"‚úÖ Configured {len(gpus)} GPU(s) with mixed precision\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"‚ö†Ô∏è GPU setup error: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU detected, using CPU (will be slow)\")\n",
    "    return len(gpus) > 0\n",
    "\n",
    "gpu_available = setup_tensorflow()"
   ],
   "metadata": {
    "id": "setup_tf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define API models\n",
    "class ImageRequest(BaseModel):\n",
    "    prompt: str = Field(..., min_length=1, max_length=500, description=\"Text description of the image\")\n",
    "    num_steps: int = Field(25, ge=10, le=50, description=\"Number of diffusion steps (lower = faster)\")\n",
    "    guidance_scale: float = Field(7.5, ge=1.0, le=15.0, description=\"How closely to follow the prompt\")\n",
    "    seed: Optional[int] = Field(None, ge=0, description=\"Random seed for reproducible results\")\n",
    "\n",
    "class ImageResponse(BaseModel):\n",
    "    image_base64: str\n",
    "    prompt: str\n",
    "    generation_time: float\n",
    "    parameters: dict\n",
    "\n",
    "print(\"‚úÖ API models defined\")"
   ],
   "metadata": {
    "id": "api_models"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create the Stable Diffusion model class\nclass ColabStableDiffusion:\n    \"\"\"Simplified Stable Diffusion model for Colab.\"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.load_model()\n\n    def load_model(self):\n        \"\"\"Load Stable Diffusion model.\"\"\"\n        print(\"üîÑ Loading Stable Diffusion model (this may take a few minutes)...\")\n\n        try:\n            self.model = keras_cv.models.StableDiffusion(\n                img_width=512,\n                img_height=512,\n                jit_compile=False,  # Disable for Colab compatibility\n            )\n            print(\"‚úÖ Stable Diffusion model loaded successfully!\")\n        except Exception as e:\n            print(f\"‚ùå Model loading failed: {e}\")\n            raise\n\n    def generate_image(self, prompt: str, num_steps: int = 25,\n                      guidance_scale: float = 7.5, seed: Optional[int] = None):\n        \"\"\"Generate image from text prompt.\"\"\"\n        if self.model is None:\n            raise RuntimeError(\"Model not loaded\")\n\n        if seed is not None:\n            tf.random.set_seed(seed)\n            np.random.seed(seed)\n\n        try:\n            print(f\"üé® Generating image for: {prompt[:50]}...\")\n            start_time = time.time()\n\n            # Check KerasCV version and use appropriate parameters\n            try:\n                # Try with guidance_scale first (newer versions)\n                generated_images = self.model.text_to_image(\n                    prompt=prompt,\n                    batch_size=1,\n                    num_steps=num_steps,\n                    guidance_scale=guidance_scale,\n                )\n            except TypeError as e:\n                if \"guidance_scale\" in str(e):\n                    print(\"‚ö†Ô∏è Using unconditional_guidance_scale parameter for older KerasCV version\")\n                    # Fallback for older versions that use unconditional_guidance_scale\n                    try:\n                        generated_images = self.model.text_to_image(\n                            prompt=prompt,\n                            batch_size=1,\n                            num_steps=num_steps,\n                            unconditional_guidance_scale=guidance_scale,\n                        )\n                    except TypeError:\n                        # If still failing, try without guidance parameter\n                        print(\"‚ö†Ô∏è Using basic parameters without guidance scale\")\n                        generated_images = self.model.text_to_image(\n                            prompt=prompt,\n                            batch_size=1,\n                            num_steps=num_steps,\n                        )\n                else:\n                    raise e\n\n            # Convert to PIL Image with proper normalization\n            img_array = generated_images[0]\n\n            # Debug: Check the actual range of values\n            print(f\"Image array range: {img_array.min():.3f} to {img_array.max():.3f}\")\n\n            # Proper normalization for KerasCV output (usually [0,1] range)\n            if img_array.max() <= 1.0 and img_array.min() >= 0.0:\n                # Standard [0,1] to [0,255] conversion\n                img_array = img_array * 255.0\n                print(\"‚úÖ Applied [0,1] ‚Üí [0,255] normalization\")\n            elif img_array.max() <= 1.0 and img_array.min() >= -1.0:\n                # [-1,1] to [0,255] conversion\n                img_array = (img_array + 1.0) * 127.5\n                print(\"‚úÖ Applied [-1,1] ‚Üí [0,255] normalization\")\n            else:\n                # Already in [0,255] range or other range - normalize to [0,255]\n                img_array = ((img_array - img_array.min()) / (img_array.max() - img_array.min())) * 255.0\n                print(\"‚úÖ Applied custom normalization to [0,255]\")\n\n            img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n            pil_image = Image.fromarray(img_array)\n\n            generation_time = time.time() - start_time\n            print(f\"‚úÖ Image generated in {generation_time:.1f} seconds\")\n\n            return pil_image\n\n        except Exception as e:\n            print(f\"‚ùå Generation failed: {e}\")\n            raise\n\n# Initialize the model (this will take a few minutes)\nprint(\"Initializing Stable Diffusion model...\")\nsd_model = ColabStableDiffusion()",
   "metadata": {
    "id": "model_class"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create FastAPI application with web interface\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import RedirectResponse\n\napp = FastAPI(\n    title=\"Text-to-Image AI (Colab)\",\n    description=\"Stable Diffusion text-to-image generation running in Google Colab\",\n    version=\"1.0.0\"\n)\n\n# Create web interface HTML content\nweb_interface_html = '''<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Text-to-Image AI Generator</title>\n    <style>\n        * { margin: 0; padding: 0; box-sizing: border-box; }\n        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; padding: 20px; }\n        .container { max-width: 800px; margin: 0 auto; background: rgba(255, 255, 255, 0.95); border-radius: 20px; padding: 30px; box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1); }\n        .header { text-align: center; margin-bottom: 30px; }\n        .header h1 { color: #333; font-size: 2.5em; margin-bottom: 10px; background: linear-gradient(45deg, #667eea, #764ba2); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }\n        .form-group { margin-bottom: 20px; }\n        .form-group label { display: block; margin-bottom: 8px; color: #333; font-weight: 600; }\n        .form-group input, .form-group textarea { width: 100%; padding: 12px; border: 2px solid #ddd; border-radius: 8px; font-size: 14px; }\n        .form-group textarea { min-height: 80px; resize: vertical; }\n        .generate-btn { width: 100%; background: linear-gradient(45deg, #667eea, #764ba2); color: white; border: none; padding: 15px; border-radius: 10px; font-size: 16px; font-weight: 600; cursor: pointer; }\n        .generate-btn:disabled { opacity: 0.6; cursor: not-allowed; }\n        .loading { display: none; text-align: center; margin: 20px 0; }\n        .loading-spinner { border: 4px solid #f3f3f3; border-top: 4px solid #667eea; border-radius: 50%; width: 40px; height: 40px; animation: spin 1s linear infinite; margin: 0 auto 20px; }\n        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }\n        .result-image { max-width: 100%; border-radius: 15px; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2); margin: 20px 0; }\n        .error-message { background: #f8d7da; color: #721c24; padding: 15px; border-radius: 10px; margin: 20px 0; }\n        .success-message { background: #d4edda; color: #155724; padding: 15px; border-radius: 10px; margin: 20px 0; }\n        .range-group { display: flex; align-items: center; gap: 10px; }\n        .range-value { background: #667eea; color: white; padding: 4px 8px; border-radius: 4px; font-size: 12px; min-width: 35px; text-align: center; }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"header\">\n            <h1>üé® Text-to-Image AI</h1>\n            <p>Create amazing images with AI-powered Stable Diffusion</p>\n        </div>\n\n        <form id=\"generateForm\">\n            <div class=\"form-group\">\n                <label for=\"prompt\">üñãÔ∏è Describe your image:</label>\n                <textarea id=\"prompt\" name=\"prompt\" placeholder=\"a beautiful sunset over mountains, serene lake, photorealistic...\" required></textarea>\n            </div>\n\n            <div class=\"form-group\">\n                <label for=\"negative_prompt\">üö´ What to avoid (optional):</label>\n                <input type=\"text\" id=\"negative_prompt\" name=\"negative_prompt\" placeholder=\"blurry, low quality, people...\">\n            </div>\n\n            <div class=\"form-group\">\n                <label for=\"num_steps\">üîÑ Steps: <span class=\"range-value\" id=\"steps-value\">25</span></label>\n                <div class=\"range-group\">\n                    <input type=\"range\" id=\"num_steps\" name=\"num_steps\" min=\"10\" max=\"50\" value=\"25\" oninput=\"document.getElementById('steps-value').textContent = this.value\">\n                </div>\n            </div>\n\n            <div class=\"form-group\">\n                <label for=\"guidance_scale\">üéØ Prompt Adherence: <span class=\"range-value\" id=\"guidance-value\">7.5</span></label>\n                <div class=\"range-group\">\n                    <input type=\"range\" id=\"guidance_scale\" name=\"guidance_scale\" min=\"1\" max=\"15\" step=\"0.5\" value=\"7.5\" oninput=\"document.getElementById('guidance-value').textContent = this.value\">\n                </div>\n            </div>\n\n            <button type=\"submit\" class=\"generate-btn\" id=\"generateBtn\">üöÄ Generate Image</button>\n        </form>\n\n        <div class=\"loading\" id=\"loading\">\n            <div class=\"loading-spinner\"></div>\n            <p>üé® Creating your masterpiece...</p>\n        </div>\n\n        <div id=\"results\"></div>\n    </div>\n\n    <script>\n        const API_BASE_URL = window.location.origin;\n\n        document.getElementById('generateForm').addEventListener('submit', async function(e) {\n            e.preventDefault();\n\n            const generateBtn = document.getElementById('generateBtn');\n            const loading = document.getElementById('loading');\n            const results = document.getElementById('results');\n\n            const formData = new FormData(e.target);\n            const requestData = {\n                prompt: formData.get('prompt'),\n                num_steps: parseInt(formData.get('num_steps')),\n                guidance_scale: parseFloat(formData.get('guidance_scale'))\n            };\n\n            if (formData.get('negative_prompt')) {\n                requestData.negative_prompt = formData.get('negative_prompt');\n            }\n\n            generateBtn.disabled = true;\n            generateBtn.textContent = 'üé® Generating...';\n            loading.style.display = 'block';\n            results.innerHTML = '';\n\n            try {\n                const response = await fetch(`${API_BASE_URL}/generate`, {\n                    method: 'POST',\n                    headers: { 'Content-Type': 'application/json' },\n                    body: JSON.stringify(requestData)\n                });\n\n                if (!response.ok) {\n                    const error = await response.json();\n                    throw new Error(error.detail || `HTTP ${response.status}`);\n                }\n\n                const result = await response.json();\n                \n                results.innerHTML = `\n                    <div class=\"success-message\">\n                        <h3>‚úÖ Image Generated Successfully!</h3>\n                        <p>Generation time: ${result.generation_time.toFixed(1)} seconds</p>\n                    </div>\n                    <img src=\"data:image/png;base64,${result.image_base64}\" alt=\"Generated Image\" class=\"result-image\">\n                `;\n\n            } catch (error) {\n                results.innerHTML = `\n                    <div class=\"error-message\">\n                        <h3>‚ùå Generation Failed</h3>\n                        <p><strong>Error:</strong> ${error.message}</p>\n                    </div>\n                `;\n            } finally {\n                generateBtn.disabled = false;\n                generateBtn.textContent = 'üöÄ Generate Image';\n                loading.style.display = 'none';\n            }\n        });\n    </script>\n</body>\n</html>'''\n\n# Store the HTML for serving\nimport os\nos.makedirs('/tmp/static', exist_ok=True)\nwith open('/tmp/static/index.html', 'w') as f:\n    f.write(web_interface_html)\n\n# Mount static files\napp.mount(\"/static\", StaticFiles(directory=\"/tmp/static\"), name=\"static\")\n\n@app.post(\"/generate\", response_model=ImageResponse)\nasync def generate_image_endpoint(request: ImageRequest):\n    \"\"\"Generate image from text prompt.\"\"\"\n    start_time = time.time()\n\n    try:\n        # Run generation in thread pool to avoid blocking\n        loop = asyncio.get_event_loop()\n        pil_image = await loop.run_in_executor(\n            None,\n            sd_model.generate_image,\n            request.prompt,\n            request.num_steps,\n            request.guidance_scale,\n            request.seed,\n        )\n\n        # Convert to base64\n        buffer = io.BytesIO()\n        pil_image.save(buffer, format=\"PNG\")\n        img_base64 = base64.b64encode(buffer.getvalue()).decode()\n\n        generation_time = time.time() - start_time\n\n        return ImageResponse(\n            image_base64=img_base64,\n            prompt=request.prompt,\n            generation_time=generation_time,\n            parameters={\n                \"num_steps\": request.num_steps,\n                \"guidance_scale\": request.guidance_scale,\n                \"seed\": request.seed\n            }\n        )\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"model_loaded\": sd_model.model is not None,\n        \"gpu_available\": gpu_available,\n        \"environment\": \"Google Colab\"\n    }\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Root endpoint - redirect to web interface.\"\"\"\n    return RedirectResponse(url=\"/static/index.html\")\n\nprint(\"‚úÖ FastAPI application with web interface created\")",
   "metadata": {
    "id": "fastapi_app"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Start the Server with Public URL"
   ],
   "metadata": {
    "id": "step5_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Start the server with ngrok tunnel\nimport threading\nimport time\nimport asyncio\nfrom uvicorn import Config, Server\n\ndef start_server():\n    \"\"\"Start the FastAPI server with proper async handling.\"\"\"\n    print(\"üîÑ Starting FastAPI server...\")\n    try:\n        # Create a new event loop for this thread\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        \n        # Configure and start the server\n        config = Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n        server = Server(config)\n        \n        # Run the server\n        loop.run_until_complete(server.serve())\n    except Exception as e:\n        print(f\"‚ùå Server startup failed: {e}\")\n\n# Start the server in a separate thread BEFORE creating ngrok tunnel\nprint(\"üöÄ Starting the FastAPI server...\")\nserver_thread = threading.Thread(target=start_server)\nserver_thread.daemon = True\nserver_thread.start()\n\n# Wait for server to start\nprint(\"‚è≥ Waiting for server to initialize...\")\ntime.sleep(15)  # Increased wait time\n\n# Test if server is running locally\nimport requests\nserver_ready = False\nfor attempt in range(5):  # Try 5 times\n    try:\n        test_response = requests.get(\"http://localhost:8000/health\", timeout=5)\n        if test_response.status_code == 200:\n            print(\"‚úÖ Server is running locally!\")\n            server_ready = True\n            break\n        else:\n            print(f\"‚ö†Ô∏è Server responded with status: {test_response.status_code}\")\n    except Exception as e:\n        print(f\"üîÑ Attempt {attempt + 1}/5: Server not ready yet...\")\n        time.sleep(5)\n\nif not server_ready:\n    print(\"‚ùå Server failed to start properly. Please restart runtime and try again.\")\nelse:\n    # Now start ngrok tunnel (uses the auth token set earlier)\n    print(\"üåê Creating ngrok tunnel...\")\n    try:\n        public_tunnel = ngrok.connect(8000)\n        public_url = str(public_tunnel)  # Convert to string URL\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"üöÄ TEXT-TO-IMAGE AI SERVICE IS LIVE!\")\n        print(\"=\"*60)\n        print(f\"üåê Web Interface: {public_url}\")\n        print(f\"üé® Direct Access: {public_url}/static/index.html\")\n        print(f\"üìñ API Documentation: {public_url}/docs\")\n        print(f\"‚ù§Ô∏è Health Check: {public_url}/health\")\n        print(f\"üîß API Generate: {public_url}/generate\")\n        print(\"=\"*60)\n        print(\"\\nüéØ HOW TO USE:\")\n        print(\"1. üñ±Ô∏è  Click the Web Interface link above\")\n        print(\"2. üñãÔ∏è  Enter your image description\")\n        print(\"3. ‚öôÔ∏è  Adjust settings as needed\")\n        print(\"4. üöÄ Click 'Generate Image'\")\n        print(\"5. ‚è≥ Wait 30-120 seconds for your AI masterpiece!\")\n        print(\"\\nüí° TIPS:\")\n        print(\"‚Ä¢ Be descriptive in your prompts\")\n        print(\"‚Ä¢ Use negative prompts to avoid unwanted elements\")\n        print(\"‚Ä¢ Lower steps = faster generation\")\n        print(\"‚Ä¢ Higher guidance = follows prompt more closely\")\n        print(\"\\n‚ö†Ô∏è Note: Keep this cell running to maintain the service\")\n        print(\"=\"*60)\n        \n    except Exception as e:\n        print(f\"‚ùå Failed to create ngrok tunnel: {e}\")\n\nif server_ready:\n    print(\"\\n‚úÖ Setup complete! Click the Web Interface link above to start creating images!\")\nelse:\n    print(\"\\n‚ùå Setup failed. Please restart runtime and try again.\")",
   "metadata": {
    "id": "start_server"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 6: Troubleshooting & Server Check\n\nIf you're getting ngrok connection errors, run this cell to diagnose the issue:",
   "metadata": {
    "id": "step6_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Troubleshooting: Check if server is running properly\nimport requests\nimport time\nimport subprocess\nimport threading\nimport asyncio\nfrom uvicorn import Config, Server\n\nprint(\"üîç Diagnosing server status...\")\n\n# Check if port 8000 is in use\ntry:\n    result = subprocess.run(['netstat', '-tuln'], capture_output=True, text=True)\n    if ':8000' in result.stdout:\n        print(\"‚úÖ Port 8000 is in use (server might be running)\")\n    else:\n        print(\"‚ùå Port 8000 is not in use (server not running)\")\nexcept:\n    print(\"‚ö†Ô∏è Could not check port status\")\n\n# Test local server connection\nprint(\"\\nüîç Testing local server connection...\")\ntry:\n    response = requests.get(\"http://localhost:8000/health\", timeout=10)\n    print(f\"‚úÖ Local server is responding! Status: {response.status_code}\")\n    print(f\"Response: {response.json()}\")\nexcept requests.exceptions.ConnectionError:\n    print(\"‚ùå Cannot connect to local server on port 8000\")\n    print(\"üîÑ Trying to restart the server...\")\n    \n    # Try to restart server with proper async handling\n    def restart_server():\n        try:\n            # Create a new event loop for this thread\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            \n            # Configure and start the server\n            config = Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n            server = Server(config)\n            \n            # Run the server\n            loop.run_until_complete(server.serve())\n        except Exception as e:\n            print(f\"‚ùå Server restart failed: {e}\")\n    \n    server_thread = threading.Thread(target=restart_server)\n    server_thread.daemon = True\n    server_thread.start()\n    \n    print(\"‚è≥ Waiting 20 seconds for server restart...\")\n    time.sleep(20)\n    \n    # Test again\n    for attempt in range(3):\n        try:\n            response = requests.get(\"http://localhost:8000/health\", timeout=5)\n            print(f\"‚úÖ Server restarted successfully! Status: {response.status_code}\")\n            break\n        except:\n            print(f\"üîÑ Restart attempt {attempt + 1}/3...\")\n            time.sleep(5)\n    else:\n        print(\"‚ùå Server restart failed\")\n\nexcept Exception as e:\n    print(f\"‚ùå Error testing server: {e}\")\n\n# Test ngrok tunnel if it exists\nif 'public_url' in globals():\n    print(f\"\\nüîç Testing ngrok tunnel: {public_url}\")\n    try:\n        response = requests.get(f\"{public_url}/health\", timeout=30)\n        print(f\"‚úÖ Ngrok tunnel is working! Status: {response.status_code}\")\n    except Exception as e:\n        print(f\"‚ùå Ngrok tunnel test failed: {e}\")\n        print(\"üí° Try running the server setup cell again\")\nelse:\n    print(\"\\n‚ö†Ô∏è No ngrok tunnel found. Run the server setup cell first.\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"üí° TROUBLESHOOTING TIPS:\")\nprint(\"=\"*50)\nprint(\"1. If local server fails: Restart runtime and run all cells again\")\nprint(\"2. If ngrok fails: Check your auth token is set correctly\")  \nprint(\"3. If still failing: Try running cells one by one with delays\")\nprint(\"4. For memory issues: Enable High-RAM runtime\")\nprint(\"5. For async errors: Make sure nest_asyncio is installed\")\nprint(\"=\"*50)",
   "metadata": {
    "id": "test_api"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Test the API (Optional)",
   "metadata": {
    "id": "step7_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Test the API directly from the notebook\nimport requests\nimport json\nfrom IPython.display import Image as IPImage\nfrom io import BytesIO\n\ndef test_generation(prompt, num_steps=25):\n    \"\"\"Test image generation directly.\"\"\"\n    print(f\"Testing generation with prompt: {prompt}\")\n    \n    # Check if we have the public URL\n    if 'public_url' not in globals():\n        print(\"‚ùå Error: No public URL found. Run the server setup cell first.\")\n        return None\n    \n    # Ensure we have a proper URL string\n    test_url = public_url if isinstance(public_url, str) else str(public_url)\n    if not test_url.startswith(('http://', 'https://')):\n        print(\"‚ùå Error: Invalid URL format\")\n        return None\n\n    try:\n        print(f\"üåê Making request to: {test_url}\")\n        \n        # Make API request\n        response = requests.post(\n            f\"{test_url}/generate\",\n            json={\n                \"prompt\": prompt,\n                \"num_steps\": num_steps,\n                \"guidance_scale\": 7.5\n            },\n            timeout=300  # 5 minute timeout for generation\n        )\n\n        if response.status_code == 200:\n            result = response.json()\n            print(f\"‚úÖ Generation completed in {result['generation_time']:.1f} seconds\")\n\n            # Decode and display image\n            img_data = base64.b64decode(result['image_base64'])\n            return IPImage(img_data)\n        else:\n            print(f\"‚ùå Error: {response.status_code} - {response.text}\")\n            return None\n            \n    except requests.exceptions.RequestException as e:\n        print(f\"‚ùå Request failed: {e}\")\n        return None\n    except Exception as e:\n        print(f\"‚ùå Unexpected error: {e}\")\n        return None\n\n# Wait a moment for everything to be ready\nprint(\"‚è≥ Waiting for services to be ready...\")\ntime.sleep(5)\n\n# Test with a simple prompt\nprint(\"üé® Testing image generation...\")\ntest_image = test_generation(\"a cute cat sitting in a garden\", num_steps=20)\nif test_image:\n    display(test_image)\n    print(\"üéâ Success! Your text-to-image service is working!\")\nelse:\n    print(\"üîÑ If the test failed, try running the troubleshooting cell above.\")\n    print(\"üí° Common solutions:\")\n    print(\"   - Wait a bit longer (model might still be loading)\")\n    print(\"   - Run the troubleshooting cell to check server status\")\n    print(\"   - Restart runtime and run all cells again\")",
   "metadata": {
    "id": "keep_running"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Keep the Service Running",
   "metadata": {
    "id": "usage_examples"
   }
  }
 ]
}