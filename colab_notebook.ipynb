{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text-to-Image AI Service in Google Colab\n",
    "\n",
    "This notebook sets up and runs a text-to-image generation service using Stable Diffusion with TensorFlow and KerasCV.\n",
    "\n",
    "## Requirements\n",
    "- Enable GPU runtime: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\n",
    "- High-RAM runtime recommended for better performance\n",
    "\n",
    "## Features\n",
    "- ğŸ¨ Stable Diffusion image generation\n",
    "- ğŸš€ FastAPI REST API\n",
    "- ğŸŒ Public URL via ngrok\n",
    "- ğŸ“± Interactive web interface"
   ],
   "metadata": {
    "id": "title_cell"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Check GPU and Setup Environment"
   ],
   "metadata": {
    "id": "step1_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Set up environment variables\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['COLAB_GPU'] = '1'\n",
    "\n",
    "print(\"âœ… Environment check complete\")"
   ],
   "metadata": {
    "id": "check_gpu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Install Dependencies"
   ],
   "metadata": {
    "id": "step2_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev libgomp1\n",
    "\n",
    "print(\"âœ… System dependencies installed\")"
   ],
   "metadata": {
    "id": "install_system"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install Python dependencies\n!pip install -q tensorflow>=2.15.0\n!pip install -q keras-cv>=0.6.0\n!pip install -q fastapi>=0.104.0\n!pip install -q uvicorn[standard]>=0.24.0\n!pip install -q python-multipart>=0.0.6\n!pip install -q pillow>=10.0.0\n!pip install -q pydantic>=2.5.0\n!pip install -q pydantic-settings>=2.1.0\n!pip install -q python-dotenv>=1.0.0\n!pip install -q structlog>=23.2.0\n!pip install -q nest-asyncio\n!pip install -q pyngrok\n!pip install -q scipy  # For image enhancement\n\nprint(\"âœ… Python dependencies installed\")",
   "metadata": {
    "id": "install_python"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Setup ngrok Authentication (REQUIRED)\n\n**IMPORTANT**: ngrok now requires authentication even for free usage.\n\n1. **Sign up for free**: Go to https://dashboard.ngrok.com/signup\n2. **Get your auth token**: Visit https://dashboard.ngrok.com/get-started/your-authtoken  \n3. **Copy the token**: It looks like `2abc123_def456ghi789jkl...`\n4. **Replace the token below**: Change `YOUR_NGROK_AUTH_TOKEN_HERE` to your actual token",
   "metadata": {
    "id": "step3_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Set up ngrok authentication (REQUIRED)\n# Get your auth token from: https://dashboard.ngrok.com/get-started/your-authtoken\n\nfrom pyngrok import ngrok, conf\n\n# REPLACE THIS with your actual ngrok auth token:\nNGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN_HERE\"\n\nif NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTH_TOKEN_HERE\":\n    print(\"âŒ ERROR: Please set your ngrok auth token!\")\n    print(\"1. Go to: https://dashboard.ngrok.com/signup (free signup)\")\n    print(\"2. Get token: https://dashboard.ngrok.com/get-started/your-authtoken\")\n    print(\"3. Replace YOUR_NGROK_AUTH_TOKEN_HERE above with your token\")\n    raise ValueError(\"ngrok auth token required\")\nelse:\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    print(\"âœ… ngrok authentication configured successfully!\")",
   "metadata": {
    "id": "setup_ngrok"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Create and Run the Text-to-Image Service"
   ],
   "metadata": {
    "id": "step4_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import Optional, List\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "# Enable nested asyncio for Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"âœ… Imports complete\")"
   ],
   "metadata": {
    "id": "imports"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Configure TensorFlow for Colab\n",
    "def setup_tensorflow():\n",
    "    \"\"\"Configure TensorFlow for Colab environment.\"\"\"\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "            print(f\"âœ… Configured {len(gpus)} GPU(s) with mixed precision\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"âš ï¸ GPU setup error: {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No GPU detected, using CPU (will be slow)\")\n",
    "    return len(gpus) > 0\n",
    "\n",
    "gpu_available = setup_tensorflow()"
   ],
   "metadata": {
    "id": "setup_tf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define enhanced API models with better defaults\nclass ImageRequest(BaseModel):\n    prompt: str = Field(..., min_length=1, max_length=500, description=\"Text description of the image\")\n    num_steps: int = Field(35, ge=15, le=50, description=\"Number of diffusion steps (higher = better quality)\")\n    guidance_scale: float = Field(12.0, ge=1.0, le=20.0, description=\"How closely to follow the prompt (higher = more adherence)\")\n    seed: Optional[int] = Field(None, ge=0, description=\"Random seed for reproducible results\")\n\nclass ImageResponse(BaseModel):\n    image_base64: str\n    prompt: str\n    enhanced_prompt: str\n    generation_time: float\n    parameters: dict\n    quality_metrics: dict\n\nprint(\"âœ… Enhanced API models defined\")",
   "metadata": {
    "id": "api_models"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create the Enhanced Stable Diffusion model class\nclass ColabStableDiffusion:\n    \"\"\"Enhanced Stable Diffusion model for better quality generation.\"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.load_model()\n\n    def load_model(self):\n        \"\"\"Load Stable Diffusion model with optimized settings.\"\"\"\n        print(\"ğŸ”„ Loading Enhanced Stable Diffusion model (this may take a few minutes)...\")\n\n        try:\n            # Use higher resolution and optimized settings\n            self.model = keras_cv.models.StableDiffusion(\n                img_width=512,\n                img_height=512,\n                jit_compile=False,  # Keep disabled for Colab compatibility\n            )\n            \n            # Warm up the model with a dummy generation for better performance\n            print(\"ğŸ”„ Warming up model for optimal performance...\")\n            try:\n                _ = self.model.text_to_image(\n                    prompt=\"test\",\n                    batch_size=1,\n                    num_steps=5,  # Minimal steps for warmup\n                )\n                print(\"âœ… Model warmed up successfully!\")\n            except Exception as warmup_error:\n                print(f\"âš ï¸ Warmup failed (non-critical): {warmup_error}\")\n            \n            print(\"âœ… Enhanced Stable Diffusion model loaded successfully!\")\n        except Exception as e:\n            print(f\"âŒ Model loading failed: {e}\")\n            raise\n\n    def generate_image(self, prompt: str, num_steps: int = 35,\n                      guidance_scale: float = 12.0, seed: Optional[int] = None):\n        \"\"\"Generate high-quality image from text prompt with enhanced settings.\"\"\"\n        if self.model is None:\n            raise RuntimeError(\"Model not loaded\")\n\n        if seed is not None:\n            tf.random.set_seed(seed)\n            np.random.seed(seed)\n\n        try:\n            # Enhanced prompt preprocessing\n            enhanced_prompt = self._enhance_prompt(prompt)\n            print(f\"ğŸ¨ Generating high-quality image for: {enhanced_prompt[:60]}...\")\n            print(f\"âš™ï¸ Settings: {num_steps} steps, guidance {guidance_scale}\")\n            \n            start_time = time.time()\n\n            # Enhanced generation parameters for better quality\n            generation_params = {\n                \"prompt\": enhanced_prompt,\n                \"batch_size\": 1,\n                \"num_steps\": max(num_steps, 25),  # Minimum 25 steps for quality\n                \"seed\": seed,\n            }\n\n            # Try with guidance_scale parameter (newer versions)\n            try:\n                generation_params[\"guidance_scale\"] = guidance_scale\n                generated_images = self.model.text_to_image(**generation_params)\n            except TypeError as e:\n                if \"guidance_scale\" in str(e):\n                    print(\"âš ï¸ Using unconditional_guidance_scale parameter for older KerasCV version\")\n                    generation_params.pop(\"guidance_scale\")\n                    generation_params[\"unconditional_guidance_scale\"] = guidance_scale\n                    try:\n                        generated_images = self.model.text_to_image(**generation_params)\n                    except TypeError:\n                        print(\"âš ï¸ Using basic parameters without guidance scale\")\n                        generation_params.pop(\"unconditional_guidance_scale\")\n                        generated_images = self.model.text_to_image(**generation_params)\n                else:\n                    raise e\n\n            # Enhanced image processing for better quality\n            img_array = generated_images[0]\n\n            # Debug output\n            print(f\"ğŸ“Š Raw image shape: {img_array.shape}\")\n            print(f\"ğŸ“Š Raw value range: {img_array.min():.4f} to {img_array.max():.4f}\")\n            print(f\"ğŸ“Š Raw dtype: {img_array.dtype}\")\n\n            # Ensure correct shape\n            if len(img_array.shape) == 4:\n                img_array = img_array[0]  # Remove batch dimension\n            \n            # Convert to float32 for precise processing\n            img_array = img_array.astype(np.float32)\n\n            # Enhanced normalization with better quality preservation\n            if img_array.min() >= 0.0 and img_array.max() <= 1.0:\n                # Standard [0,1] to [0,255] with gamma correction for better contrast\n                img_array = np.power(img_array, 0.9)  # Slight gamma correction\n                img_array = img_array * 255.0\n                print(\"âœ… Applied enhanced [0,1] â†’ [0,255] normalization with gamma correction\")\n            elif img_array.min() >= -1.0 and img_array.max() <= 1.0:\n                # Enhanced [-1,1] to [0,255] conversion\n                img_array = (img_array + 1.0) / 2.0  # Convert to [0,1]\n                img_array = np.power(img_array, 0.9)  # Gamma correction\n                img_array = img_array * 255.0\n                print(\"âœ… Applied enhanced [-1,1] â†’ [0,255] normalization with gamma correction\")\n            else:\n                # Robust normalization with contrast enhancement\n                img_min, img_max = img_array.min(), img_array.max()\n                if img_max > img_min:\n                    img_array = (img_array - img_min) / (img_max - img_min)\n                    # Apply contrast enhancement\n                    img_array = np.power(img_array, 0.9)\n                    img_array = img_array * 255.0\n                else:\n                    img_array = np.zeros_like(img_array)\n                print(f\"âœ… Applied custom normalization with contrast enhancement\")\n\n            # Enhanced post-processing for better quality\n            img_array = self._enhance_image_quality(img_array)\n            \n            # Final clipping and conversion\n            img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n            \n            # Quality check\n            unique_values = len(np.unique(img_array))\n            print(f\"ğŸ“Š Final image: {img_array.shape}, {unique_values} unique values\")\n            \n            if unique_values < 50:\n                print(\"âš ï¸ Warning: Low color diversity detected\")\n            else:\n                print(\"âœ… Good color diversity achieved\")\n\n            # Create high-quality PIL image\n            pil_image = Image.fromarray(img_array, mode='RGB')\n            \n            generation_time = time.time() - start_time\n            print(f\"âœ… High-quality image generated in {generation_time:.1f} seconds\")\n\n            return pil_image\n\n        except Exception as e:\n            print(f\"âŒ Generation failed: {e}\")\n            import traceback\n            traceback.print_exc()\n            raise\n\n    def _enhance_prompt(self, prompt: str) -> str:\n        \"\"\"Enhance the prompt for better quality results.\"\"\"\n        # Add quality enhancing keywords if not already present\n        quality_keywords = [\n            \"high quality\", \"detailed\", \"sharp\", \"clear\", \"professional\",\n            \"8k\", \"hd\", \"masterpiece\", \"best quality\"\n        ]\n        \n        prompt_lower = prompt.lower()\n        has_quality_terms = any(keyword in prompt_lower for keyword in quality_keywords)\n        \n        if not has_quality_terms:\n            prompt = f\"{prompt}, high quality, detailed, sharp focus, professional\"\n        \n        # Remove common quality-degrading terms\n        negative_terms = [\"blurry\", \"low quality\", \"pixelated\", \"jpeg artifacts\"]\n        for term in negative_terms:\n            prompt = prompt.replace(term, \"\").replace(term.capitalize(), \"\")\n        \n        return prompt.strip()\n\n    def _enhance_image_quality(self, img_array: np.ndarray) -> np.ndarray:\n        \"\"\"Apply post-processing to enhance image quality.\"\"\"\n        # Slight sharpening using unsharp mask technique\n        from scipy import ndimage\n        \n        try:\n            # Create a slightly blurred version\n            blurred = ndimage.gaussian_filter(img_array, sigma=0.5)\n            \n            # Create sharpening mask\n            sharpening_strength = 0.2\n            enhanced = img_array + sharpening_strength * (img_array - blurred)\n            \n            # Ensure values stay in valid range\n            enhanced = np.clip(enhanced, 0, 255)\n            \n            print(\"âœ… Applied subtle sharpening enhancement\")\n            return enhanced\n        except ImportError:\n            print(\"â„¹ï¸ Scipy not available, skipping sharpening enhancement\")\n            return img_array\n        except Exception as e:\n            print(f\"âš ï¸ Enhancement failed: {e}, using original\")\n            return img_array\n\n# Initialize the enhanced model\nprint(\"ğŸš€ Initializing Enhanced Stable Diffusion model...\")\nsd_model = ColabStableDiffusion()",
   "metadata": {
    "id": "model_class"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create FastAPI application with web interface\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import RedirectResponse\n\napp = FastAPI(\n    title=\"Enhanced Text-to-Image AI (Colab)\",\n    description=\"High-quality Stable Diffusion text-to-image generation running in Google Colab\",\n    version=\"2.0.0\"\n)\n\n# Store the HTML for serving (will be updated later with enhanced interface)\nimport os\nos.makedirs('/tmp/static', exist_ok=True)\n\n# Mount static files\napp.mount(\"/static\", StaticFiles(directory=\"/tmp/static\"), name=\"static\")\n\n@app.post(\"/generate\", response_model=ImageResponse)\nasync def generate_image_endpoint(request: ImageRequest):\n    \"\"\"Generate high-quality image from text prompt.\"\"\"\n    start_time = time.time()\n\n    try:\n        # Get enhanced prompt for quality metrics\n        enhanced_prompt = sd_model._enhance_prompt(request.prompt)\n\n        # Run generation in thread pool to avoid blocking\n        loop = asyncio.get_event_loop()\n        pil_image = await loop.run_in_executor(\n            None,\n            sd_model.generate_image,\n            request.prompt,\n            request.num_steps,\n            request.guidance_scale,\n            request.seed,\n        )\n\n        # Convert to base64 with high quality PNG\n        buffer = io.BytesIO()\n        pil_image.save(buffer, format=\"PNG\", optimize=False, compress_level=1)\n        img_base64 = base64.b64encode(buffer.getvalue()).decode()\n\n        generation_time = time.time() - start_time\n\n        # Calculate quality metrics\n        img_array = np.array(pil_image)\n        quality_metrics = {\n            \"unique_colors\": int(len(np.unique(img_array))),\n            \"image_size\": pil_image.size,\n            \"color_channels\": len(img_array.shape),\n            \"dynamic_range\": f\"{img_array.min()}-{img_array.max()}\",\n            \"enhancement_applied\": True\n        }\n\n        return ImageResponse(\n            image_base64=img_base64,\n            prompt=request.prompt,\n            enhanced_prompt=enhanced_prompt,\n            generation_time=generation_time,\n            parameters={\n                \"num_steps\": request.num_steps,\n                \"guidance_scale\": request.guidance_scale,\n                \"seed\": request.seed\n            },\n            quality_metrics=quality_metrics\n        )\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"model_loaded\": sd_model.model is not None,\n        \"gpu_available\": gpu_available,\n        \"environment\": \"Google Colab\",\n        \"version\": \"2.0.0\",\n        \"enhancements\": [\"quality_mode\", \"gamma_correction\", \"sharpening\", \"prompt_enhancement\"]\n    }\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Root endpoint - redirect to web interface.\"\"\"\n    return RedirectResponse(url=\"/static/index.html\")\n\nprint(\"âœ… Enhanced FastAPI application created\")",
   "metadata": {
    "id": "fastapi_app"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Enhanced web interface with better quality defaults\nenhanced_web_interface_html = '''<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Enhanced Text-to-Image AI Generator</title>\n    <style>\n        * { margin: 0; padding: 0; box-sizing: border-box; }\n        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; padding: 20px; }\n        .container { max-width: 900px; margin: 0 auto; background: rgba(255, 255, 255, 0.95); border-radius: 20px; padding: 30px; box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1); }\n        .header { text-align: center; margin-bottom: 30px; }\n        .header h1 { color: #333; font-size: 2.5em; margin-bottom: 10px; background: linear-gradient(45deg, #667eea, #764ba2); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }\n        .header .subtitle { color: #666; font-size: 1.1em; }\n        .quality-badge { display: inline-block; background: linear-gradient(45deg, #28a745, #20c997); color: white; padding: 5px 12px; border-radius: 15px; font-size: 0.9em; margin-top: 10px; }\n        .form-group { margin-bottom: 20px; }\n        .form-group label { display: block; margin-bottom: 8px; color: #333; font-weight: 600; }\n        .form-group input, .form-group textarea { width: 100%; padding: 12px; border: 2px solid #ddd; border-radius: 8px; font-size: 14px; }\n        .form-group textarea { min-height: 80px; resize: vertical; }\n        .generate-btn { width: 100%; background: linear-gradient(45deg, #667eea, #764ba2); color: white; border: none; padding: 15px; border-radius: 10px; font-size: 16px; font-weight: 600; cursor: pointer; transition: all 0.3s; }\n        .generate-btn:hover { transform: translateY(-2px); box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4); }\n        .generate-btn:disabled { opacity: 0.6; cursor: not-allowed; transform: none; }\n        .loading { display: none; text-align: center; margin: 20px 0; }\n        .loading-spinner { border: 4px solid #f3f3f3; border-top: 4px solid #667eea; border-radius: 50%; width: 40px; height: 40px; animation: spin 1s linear infinite; margin: 0 auto 20px; }\n        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }\n        .result-image { max-width: 100%; border-radius: 15px; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2); margin: 20px 0; }\n        .error-message { background: #f8d7da; color: #721c24; padding: 15px; border-radius: 10px; margin: 20px 0; }\n        .success-message { background: #d4edda; color: #155724; padding: 15px; border-radius: 10px; margin: 20px 0; }\n        .quality-metrics { background: #e7f3ff; color: #0c5460; padding: 15px; border-radius: 10px; margin: 10px 0; font-size: 0.9em; }\n        .range-group { display: flex; align-items: center; gap: 10px; }\n        .range-value { background: #667eea; color: white; padding: 4px 8px; border-radius: 4px; font-size: 12px; min-width: 35px; text-align: center; }\n        .quality-indicator { font-size: 0.8em; color: #666; margin-left: 10px; }\n        .tips { background: #fff3cd; color: #856404; padding: 15px; border-radius: 10px; margin: 20px 0; }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"header\">\n            <h1>ğŸ¨ Enhanced AI Image Generator</h1>\n            <p class=\"subtitle\">High-quality images with AI-powered Stable Diffusion</p>\n            <span class=\"quality-badge\">âœ¨ Enhanced Quality Mode</span>\n        </div>\n\n        <div class=\"tips\">\n            <strong>ğŸ’¡ Quality Tips:</strong>\n            <ul style=\"margin: 10px 0; padding-left: 20px;\">\n                <li>Be specific and descriptive in your prompts</li>\n                <li>Higher steps = better quality (35+ recommended)</li>\n                <li>Higher guidance = closer prompt adherence</li>\n                <li>Add quality terms like \"detailed, sharp, professional\"</li>\n            </ul>\n        </div>\n\n        <form id=\"generateForm\">\n            <div class=\"form-group\">\n                <label for=\"prompt\">ğŸ–‹ï¸ Describe your image:</label>\n                <textarea id=\"prompt\" name=\"prompt\" placeholder=\"a photorealistic portrait of a majestic lion, detailed fur texture, golden hour lighting, professional photography...\" required></textarea>\n            </div>\n\n            <div class=\"form-group\">\n                <label for=\"num_steps\">ğŸ”„ Quality Steps: <span class=\"range-value\" id=\"steps-value\">35</span></label>\n                <div class=\"range-group\">\n                    <input type=\"range\" id=\"num_steps\" name=\"num_steps\" min=\"15\" max=\"50\" value=\"35\" oninput=\"updateStepsValue(this.value)\">\n                    <span class=\"quality-indicator\" id=\"steps-indicator\">High Quality</span>\n                </div>\n            </div>\n\n            <div class=\"form-group\">\n                <label for=\"guidance_scale\">ğŸ¯ Prompt Adherence: <span class=\"range-value\" id=\"guidance-value\">12.0</span></label>\n                <div class=\"range-group\">\n                    <input type=\"range\" id=\"guidance_scale\" name=\"guidance_scale\" min=\"1\" max=\"20\" step=\"0.5\" value=\"12.0\" oninput=\"updateGuidanceValue(this.value)\">\n                    <span class=\"quality-indicator\" id=\"guidance-indicator\">Strong</span>\n                </div>\n            </div>\n\n            <button type=\"submit\" class=\"generate-btn\" id=\"generateBtn\">ğŸš€ Generate High-Quality Image</button>\n        </form>\n\n        <div class=\"loading\" id=\"loading\">\n            <div class=\"loading-spinner\"></div>\n            <p>ğŸ¨ Creating your high-quality masterpiece...</p>\n        </div>\n\n        <div id=\"results\"></div>\n    </div>\n\n    <script>\n        const API_BASE_URL = window.location.origin;\n\n        function updateStepsValue(value) {\n            document.getElementById('steps-value').textContent = value;\n            const indicator = document.getElementById('steps-indicator');\n            if (value < 20) indicator.textContent = 'Fast';\n            else if (value < 30) indicator.textContent = 'Good';\n            else if (value < 40) indicator.textContent = 'High Quality';\n            else indicator.textContent = 'Maximum Quality';\n        }\n\n        function updateGuidanceValue(value) {\n            document.getElementById('guidance-value').textContent = value;\n            const indicator = document.getElementById('guidance-indicator');\n            if (value < 5) indicator.textContent = 'Creative';\n            else if (value < 10) indicator.textContent = 'Balanced';\n            else if (value < 15) indicator.textContent = 'Strong';\n            else indicator.textContent = 'Very Strong';\n        }\n\n        document.getElementById('generateForm').addEventListener('submit', async function(e) {\n            e.preventDefault();\n\n            const generateBtn = document.getElementById('generateBtn');\n            const loading = document.getElementById('loading');\n            const results = document.getElementById('results');\n\n            const formData = new FormData(e.target);\n            const requestData = {\n                prompt: formData.get('prompt'),\n                num_steps: parseInt(formData.get('num_steps')),\n                guidance_scale: parseFloat(formData.get('guidance_scale'))\n            };\n\n            generateBtn.disabled = true;\n            generateBtn.textContent = 'ğŸ¨ Generating...';\n            loading.style.display = 'block';\n            results.innerHTML = '';\n\n            try {\n                const response = await fetch(`${API_BASE_URL}/generate`, {\n                    method: 'POST',\n                    headers: { 'Content-Type': 'application/json' },\n                    body: JSON.stringify(requestData)\n                });\n\n                if (!response.ok) {\n                    const error = await response.json();\n                    throw new Error(error.detail || `HTTP ${response.status}`);\n                }\n\n                const result = await response.json();\n                \n                const qualityInfo = result.quality_metrics ? `\n                    <div class=\"quality-metrics\">\n                        <strong>ğŸ“Š Quality Metrics:</strong><br>\n                        ğŸ¨ Unique Colors: ${result.quality_metrics.unique_colors}<br>\n                        ğŸ“ Image Size: ${result.quality_metrics.image_size[0]}Ã—${result.quality_metrics.image_size[1]}<br>\n                        âœ¨ Enhanced: ${result.quality_metrics.enhancement_applied ? 'Yes' : 'No'}<br>\n                        ğŸ¯ Enhanced Prompt: \"${result.enhanced_prompt}\"\n                    </div>\n                ` : '';\n                \n                results.innerHTML = `\n                    <div class=\"success-message\">\n                        <h3>âœ… High-Quality Image Generated!</h3>\n                        <p>Generation time: ${result.generation_time.toFixed(1)} seconds</p>\n                        <p>Steps: ${result.parameters.num_steps} | Guidance: ${result.parameters.guidance_scale}</p>\n                    </div>\n                    ${qualityInfo}\n                    <img src=\"data:image/png;base64,${result.image_base64}\" alt=\"Generated Image\" class=\"result-image\">\n                `;\n\n            } catch (error) {\n                results.innerHTML = `\n                    <div class=\"error-message\">\n                        <h3>âŒ Generation Failed</h3>\n                        <p><strong>Error:</strong> ${error.message}</p>\n                    </div>\n                `;\n            } finally {\n                generateBtn.disabled = false;\n                generateBtn.textContent = 'ğŸš€ Generate High-Quality Image';\n                loading.style.display = 'none';\n            }\n        });\n    </script>\n</body>\n</html>'''\n\n# Update the static HTML file with enhanced interface\nwith open('/tmp/static/index.html', 'w') as f:\n    f.write(enhanced_web_interface_html)\n\nprint(\"âœ… Enhanced web interface with quality improvements created\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Start the Server with Public URL"
   ],
   "metadata": {
    "id": "step5_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Start the server with ngrok tunnel\nimport threading\nimport time\nimport asyncio\nfrom uvicorn import Config, Server\n\ndef start_server():\n    \"\"\"Start the FastAPI server with proper async handling.\"\"\"\n    print(\"ğŸ”„ Starting FastAPI server...\")\n    try:\n        # Create a new event loop for this thread\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        \n        # Configure and start the server\n        config = Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n        server = Server(config)\n        \n        # Run the server\n        loop.run_until_complete(server.serve())\n    except Exception as e:\n        print(f\"âŒ Server startup failed: {e}\")\n\n# Start the server in a separate thread BEFORE creating ngrok tunnel\nprint(\"ğŸš€ Starting the FastAPI server...\")\nserver_thread = threading.Thread(target=start_server)\nserver_thread.daemon = True\nserver_thread.start()\n\n# Wait for server to start\nprint(\"â³ Waiting for server to initialize...\")\ntime.sleep(15)  # Increased wait time\n\n# Test if server is running locally\nimport requests\nserver_ready = False\nfor attempt in range(5):  # Try 5 times\n    try:\n        test_response = requests.get(\"http://localhost:8000/health\", timeout=5)\n        if test_response.status_code == 200:\n            print(\"âœ… Server is running locally!\")\n            server_ready = True\n            break\n        else:\n            print(f\"âš ï¸ Server responded with status: {test_response.status_code}\")\n    except Exception as e:\n        print(f\"ğŸ”„ Attempt {attempt + 1}/5: Server not ready yet...\")\n        time.sleep(5)\n\nif not server_ready:\n    print(\"âŒ Server failed to start properly. Please restart runtime and try again.\")\nelse:\n    # Now start ngrok tunnel (uses the auth token set earlier)\n    print(\"ğŸŒ Creating ngrok tunnel...\")\n    try:\n        public_tunnel = ngrok.connect(8000)\n        public_url = str(public_tunnel)  # Convert to string URL\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"ğŸš€ TEXT-TO-IMAGE AI SERVICE IS LIVE!\")\n        print(\"=\"*60)\n        print(f\"ğŸŒ Web Interface: {public_url}\")\n        print(f\"ğŸ¨ Direct Access: {public_url}/static/index.html\")\n        print(f\"ğŸ“– API Documentation: {public_url}/docs\")\n        print(f\"â¤ï¸ Health Check: {public_url}/health\")\n        print(f\"ğŸ”§ API Generate: {public_url}/generate\")\n        print(\"=\"*60)\n        print(\"\\nğŸ¯ HOW TO USE:\")\n        print(\"1. ğŸ–±ï¸  Click the Web Interface link above\")\n        print(\"2. ğŸ–‹ï¸  Enter your image description\")\n        print(\"3. âš™ï¸  Adjust settings as needed\")\n        print(\"4. ğŸš€ Click 'Generate Image'\")\n        print(\"5. â³ Wait 30-120 seconds for your AI masterpiece!\")\n        print(\"\\nğŸ’¡ TIPS:\")\n        print(\"â€¢ Be descriptive in your prompts\")\n        print(\"â€¢ Use negative prompts to avoid unwanted elements\")\n        print(\"â€¢ Lower steps = faster generation\")\n        print(\"â€¢ Higher guidance = follows prompt more closely\")\n        print(\"\\nâš ï¸ Note: Keep this cell running to maintain the service\")\n        print(\"=\"*60)\n        \n    except Exception as e:\n        print(f\"âŒ Failed to create ngrok tunnel: {e}\")\n\nif server_ready:\n    print(\"\\nâœ… Setup complete! Click the Web Interface link above to start creating images!\")\nelse:\n    print(\"\\nâŒ Setup failed. Please restart runtime and try again.\")",
   "metadata": {
    "id": "start_server"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 6: Troubleshooting & Server Check\n\nIf you're getting ngrok connection errors, run this cell to diagnose the issue:",
   "metadata": {
    "id": "step6_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Troubleshooting: Check if server is running properly\nimport requests\nimport time\nimport subprocess\nimport threading\nimport asyncio\nfrom uvicorn import Config, Server\n\nprint(\"ğŸ” Diagnosing server status...\")\n\n# Check if port 8000 is in use\ntry:\n    result = subprocess.run(['netstat', '-tuln'], capture_output=True, text=True)\n    if ':8000' in result.stdout:\n        print(\"âœ… Port 8000 is in use (server might be running)\")\n    else:\n        print(\"âŒ Port 8000 is not in use (server not running)\")\nexcept:\n    print(\"âš ï¸ Could not check port status\")\n\n# Test local server connection\nprint(\"\\nğŸ” Testing local server connection...\")\ntry:\n    response = requests.get(\"http://localhost:8000/health\", timeout=10)\n    print(f\"âœ… Local server is responding! Status: {response.status_code}\")\n    print(f\"Response: {response.json()}\")\nexcept requests.exceptions.ConnectionError:\n    print(\"âŒ Cannot connect to local server on port 8000\")\n    print(\"ğŸ”„ Trying to restart the server...\")\n    \n    # Try to restart server with proper async handling\n    def restart_server():\n        try:\n            # Create a new event loop for this thread\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            \n            # Configure and start the server\n            config = Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n            server = Server(config)\n            \n            # Run the server\n            loop.run_until_complete(server.serve())\n        except Exception as e:\n            print(f\"âŒ Server restart failed: {e}\")\n    \n    server_thread = threading.Thread(target=restart_server)\n    server_thread.daemon = True\n    server_thread.start()\n    \n    print(\"â³ Waiting 20 seconds for server restart...\")\n    time.sleep(20)\n    \n    # Test again\n    for attempt in range(3):\n        try:\n            response = requests.get(\"http://localhost:8000/health\", timeout=5)\n            print(f\"âœ… Server restarted successfully! Status: {response.status_code}\")\n            break\n        except:\n            print(f\"ğŸ”„ Restart attempt {attempt + 1}/3...\")\n            time.sleep(5)\n    else:\n        print(\"âŒ Server restart failed\")\n\nexcept Exception as e:\n    print(f\"âŒ Error testing server: {e}\")\n\n# Test ngrok tunnel if it exists\nif 'public_url' in globals():\n    print(f\"\\nğŸ” Testing ngrok tunnel: {public_url}\")\n    try:\n        response = requests.get(f\"{public_url}/health\", timeout=30)\n        print(f\"âœ… Ngrok tunnel is working! Status: {response.status_code}\")\n    except Exception as e:\n        print(f\"âŒ Ngrok tunnel test failed: {e}\")\n        print(\"ğŸ’¡ Try running the server setup cell again\")\nelse:\n    print(\"\\nâš ï¸ No ngrok tunnel found. Run the server setup cell first.\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"ğŸ’¡ TROUBLESHOOTING TIPS:\")\nprint(\"=\"*50)\nprint(\"1. If local server fails: Restart runtime and run all cells again\")\nprint(\"2. If ngrok fails: Check your auth token is set correctly\")  \nprint(\"3. If still failing: Try running cells one by one with delays\")\nprint(\"4. For memory issues: Enable High-RAM runtime\")\nprint(\"5. For async errors: Make sure nest_asyncio is installed\")\nprint(\"=\"*50)",
   "metadata": {
    "id": "test_api"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Test the API (Optional)",
   "metadata": {
    "id": "step7_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Test the API directly from the notebook\nimport requests\nimport json\nfrom IPython.display import Image as IPImage\nfrom io import BytesIO\n\ndef test_generation(prompt, num_steps=25):\n    \"\"\"Test image generation directly.\"\"\"\n    print(f\"Testing generation with prompt: {prompt}\")\n    \n    # Check if we have the public URL\n    if 'public_url' not in globals():\n        print(\"âŒ Error: No public URL found. Run the server setup cell first.\")\n        return None\n    \n    # Ensure we have a proper URL string\n    test_url = public_url if isinstance(public_url, str) else str(public_url)\n    if not test_url.startswith(('http://', 'https://')):\n        print(\"âŒ Error: Invalid URL format\")\n        return None\n\n    try:\n        print(f\"ğŸŒ Making request to: {test_url}\")\n        \n        # Make API request\n        response = requests.post(\n            f\"{test_url}/generate\",\n            json={\n                \"prompt\": prompt,\n                \"num_steps\": num_steps,\n                \"guidance_scale\": 7.5\n            },\n            timeout=300  # 5 minute timeout for generation\n        )\n\n        if response.status_code == 200:\n            result = response.json()\n            print(f\"âœ… Generation completed in {result['generation_time']:.1f} seconds\")\n\n            # Decode and display image\n            img_data = base64.b64decode(result['image_base64'])\n            return IPImage(img_data)\n        else:\n            print(f\"âŒ Error: {response.status_code} - {response.text}\")\n            return None\n            \n    except requests.exceptions.RequestException as e:\n        print(f\"âŒ Request failed: {e}\")\n        return None\n    except Exception as e:\n        print(f\"âŒ Unexpected error: {e}\")\n        return None\n\n# Wait a moment for everything to be ready\nprint(\"â³ Waiting for services to be ready...\")\ntime.sleep(5)\n\n# Test with a simple prompt\nprint(\"ğŸ¨ Testing image generation...\")\ntest_image = test_generation(\"a cute cat sitting in a garden\", num_steps=20)\nif test_image:\n    display(test_image)\n    print(\"ğŸ‰ Success! Your text-to-image service is working!\")\nelse:\n    print(\"ğŸ”„ If the test failed, try running the troubleshooting cell above.\")\n    print(\"ğŸ’¡ Common solutions:\")\n    print(\"   - Wait a bit longer (model might still be loading)\")\n    print(\"   - Run the troubleshooting cell to check server status\")\n    print(\"   - Restart runtime and run all cells again\")",
   "metadata": {
    "id": "keep_running"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Keep the Service Running",
   "metadata": {
    "id": "usage_examples"
   }
  },
  {
   "cell_type": "code",
   "source": "# DEBUGGING: Test image generation with detailed output\nimport requests\nimport base64\nimport subprocess\nfrom PIL import Image\nimport io\n\ndef debug_generation():\n    \"\"\"Debug test with corrected URL extraction\"\"\"\n    print(\"ğŸ” DEBUG: Testing image generation with detailed output...\")\n    \n    # Get the correct ngrok URL (just the HTTPS part)\n    try:\n        result = subprocess.run(['ngrok', 'tunnel', 'list'], capture_output=True, text=True)\n        \n        # Extract just the HTTPS URL from the tunnel list\n        ngrok_url = None\n        for line in result.stdout.split('\\n'):\n            if 'https://' in line and 'localhost:8000' in line:\n                # Extract just the https:// URL part\n                parts = line.split()\n                for part in parts:\n                    if part.startswith('https://') and '.ngrok-free.dev' in part:\n                        ngrok_url = part.strip()\n                        break\n                break\n        \n        if not ngrok_url:\n            print(\"âŒ No active ngrok tunnel found!\")\n            print(\"Run: !ngrok http 8000 --log stdout\")\n            return\n        \n        print(f\"âœ… Using ngrok URL: {ngrok_url}\")\n        \n        # Test the /generate endpoint\n        url = f\"{ngrok_url}/generate\"\n        \n        test_data = {\n            \"prompt\": \"a simple red circle on white background, minimal, clean\",\n            \"num_steps\": 10,  # Minimal steps for faster testing\n            \"guidance_scale\": 7.5\n        }\n        \n        print(f\"ğŸ” Testing URL: {url}\")\n        print(f\"ğŸ“ Request data: {test_data}\")\n        \n        response = requests.post(\n            url,\n            json=test_data,\n            headers={'Content-Type': 'application/json'},\n            timeout=120\n        )\n        \n        print(f\"ğŸ“Š Response status: {response.status_code}\")\n        print(f\"ğŸ“‹ Response headers: {dict(response.headers)}\")\n        \n        if response.status_code == 200:\n            result = response.json()\n            print(\"âœ… Generation successful!\")\n            print(f\"ğŸ¯ Prompt: {result.get('prompt', 'N/A')}\")\n            print(f\"â±ï¸ Time: {result.get('generation_time', 'N/A')} seconds\")\n            \n            # Test image decoding\n            if 'image_base64' in result:\n                try:\n                    img_data = base64.b64decode(result['image_base64'])\n                    img = Image.open(io.BytesIO(img_data))\n                    print(f\"ğŸ–¼ï¸ Image size: {img.size}\")\n                    print(f\"ğŸ¨ Image mode: {img.mode}\")\n                    print(\"âœ… Image successfully decoded!\")\n                    \n                    # Display the image\n                    from IPython.display import Image as IPImage\n                    display(IPImage(img_data))\n                    return True\n                except Exception as e:\n                    print(f\"âŒ Image decode error: {e}\")\n                    return False\n            else:\n                print(\"âŒ No image_base64 in response\")\n                return False\n        else:\n            print(f\"âŒ Request failed: {response.status_code}\")\n            try:\n                error_detail = response.json()\n                print(f\"ğŸ“„ Error details: {error_detail}\")\n            except:\n                print(f\"ğŸ“„ Response text: {response.text}\")\n            return False\n            \n    except Exception as e:\n        print(f\"âŒ Debug test failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\n# Run the debug test\nprint(\"ğŸš€ Running corrected debug test...\")\ndebug_success = debug_generation()\n\nif debug_success:\n    print(\"\\nğŸ‰ Debug test completed successfully!\")\n    print(\"âœ… Your ngrok tunnel and image generation are working properly!\")\nelse:\n    print(\"\\nğŸ’¡ Debug test failed. Common solutions:\")\n    print(\"1. Make sure ngrok tunnel is active: !ngrok tunnel list\")\n    print(\"2. If no tunnel, create one: !ngrok http 8000 --log stdout\")\n    print(\"3. Check if the FastAPI server is running on port 8000\")\n    print(\"4. If server stopped, re-run the server setup cell\")\n    print(\"5. Replace the old ngrok URL in your web interface with the new one\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}