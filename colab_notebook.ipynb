{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text-to-Image AI Service in Google Colab\n",
        "\n",
        "This notebook sets up and runs a text-to-image generation service using Stable Diffusion with TensorFlow and KerasCV.\n",
        "\n",
        "## Requirements\n",
        "- Enable GPU runtime: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n",
        "- High-RAM runtime recommended for better performance\n",
        "\n",
        "## Features\n",
        "- üé® Stable Diffusion image generation\n",
        "- üöÄ FastAPI REST API\n",
        "- üåê Public URL via ngrok\n",
        "- üì± Interactive web interface"
      ],
      "metadata": {
        "id": "title_cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Check GPU and Setup Environment"
      ],
      "metadata": {
        "id": "step1_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Set up environment variables\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "os.environ['COLAB_GPU'] = '1'\n",
        "\n",
        "print(\"‚úÖ Environment check complete\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Install Dependencies"
      ],
      "metadata": {
        "id": "step2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install system dependencies\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev libgomp1\n",
        "\n",
        "print(\"‚úÖ System dependencies installed\")"
      ],
      "metadata": {
        "id": "install_system"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Python dependencies\n",
        "!pip install -q tensorflow>=2.15.0\n",
        "!pip install -q keras-cv>=0.6.0\n",
        "!pip install -q fastapi>=0.104.0\n",
        "!pip install -q uvicorn[standard]>=0.24.0\n",
        "!pip install -q python-multipart>=0.0.6\n",
        "!pip install -q pillow>=10.0.0\n",
        "!pip install -q pydantic>=2.5.0\n",
        "!pip install -q pydantic-settings>=2.1.0\n",
        "!pip install -q python-dotenv>=1.0.0\n",
        "!pip install -q structlog>=23.2.0\n",
        "!pip install -q nest-asyncio\n",
        "!pip install -q pyngrok\n",
        "\n",
        "print(\"‚úÖ Python dependencies installed\")"
      ],
      "metadata": {
        "id": "install_python"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Setup Authentication for ngrok (Optional but Recommended)"
      ],
      "metadata": {
        "id": "step3_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up ngrok authentication (optional but recommended for stable URLs)\n",
        "# Get your auth token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# Uncomment and add your ngrok auth token for better reliability:\n",
        "# ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN_HERE\")\n",
        "\n",
        "print(\"‚úÖ ngrok configured (add auth token for better reliability)\")"
      ],
      "metadata": {
        "id": "setup_ngrok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Create and Run the Text-to-Image Service"
      ],
      "metadata": {
        "id": "step4_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from typing import Optional, List\n",
        "import tensorflow as tf\n",
        "import keras_cv\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel, Field\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n",
        "# Enable nested asyncio for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"‚úÖ Imports complete\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure TensorFlow for Colab\n",
        "def setup_tensorflow():\n",
        "    \"\"\"Configure TensorFlow for Colab environment.\"\"\"\n",
        "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "            print(f\"‚úÖ Configured {len(gpus)} GPU(s) with mixed precision\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"‚ö†Ô∏è GPU setup error: {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No GPU detected, using CPU (will be slow)\")\n",
        "    return len(gpus) > 0\n",
        "\n",
        "gpu_available = setup_tensorflow()"
      ],
      "metadata": {
        "id": "setup_tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define API models\n",
        "class ImageRequest(BaseModel):\n",
        "    prompt: str = Field(..., min_length=1, max_length=500, description=\"Text description of the image\")\n",
        "    num_steps: int = Field(25, ge=10, le=50, description=\"Number of diffusion steps (lower = faster)\")\n",
        "    guidance_scale: float = Field(7.5, ge=1.0, le=15.0, description=\"How closely to follow the prompt\")\n",
        "    seed: Optional[int] = Field(None, ge=0, description=\"Random seed for reproducible results\")\n",
        "\n",
        "class ImageResponse(BaseModel):\n",
        "    image_base64: str\n",
        "    prompt: str\n",
        "    generation_time: float\n",
        "    parameters: dict\n",
        "\n",
        "print(\"‚úÖ API models defined\")"
      ],
      "metadata": {
        "id": "api_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Stable Diffusion model class\n",
        "class ColabStableDiffusion:\n",
        "    \"\"\"Simplified Stable Diffusion model for Colab.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load Stable Diffusion model.\"\"\"\n",
        "        print(\"üîÑ Loading Stable Diffusion model (this may take a few minutes)...\")\n",
        "\n",
        "        try:\n",
        "            self.model = keras_cv.models.StableDiffusion(\n",
        "                img_width=512,\n",
        "                img_height=512,\n",
        "                jit_compile=False,  # Disable for Colab compatibility\n",
        "            )\n",
        "            print(\"‚úÖ Stable Diffusion model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Model loading failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate_image(self, prompt: str, num_steps: int = 25,\n",
        "                      guidance_scale: float = 7.5, seed: Optional[int] = None):\n",
        "        \"\"\"Generate image from text prompt.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"Model not loaded\")\n",
        "\n",
        "        if seed is not None:\n",
        "            tf.random.set_seed(seed)\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        try:\n",
        "            print(f\"üé® Generating image for: {prompt[:50]}...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            generated_images = self.model.text_to_image(\n",
        "                prompt=prompt,\n",
        "                batch_size=1,\n",
        "                num_steps=num_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "            )\n",
        "\n",
        "            # Convert to PIL Image\n",
        "            img_array = generated_images[0]\n",
        "            img_array = (img_array + 1.0) * 127.5\n",
        "            img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n",
        "            pil_image = Image.fromarray(img_array)\n",
        "\n",
        "            generation_time = time.time() - start_time\n",
        "            print(f\"‚úÖ Image generated in {generation_time:.1f} seconds\")\n",
        "\n",
        "            return pil_image\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Generation failed: {e}\")\n",
        "            raise\n",
        "\n",
        "# Initialize the model (this will take a few minutes)\n",
        "print(\"Initializing Stable Diffusion model...\")\n",
        "sd_model = ColabStableDiffusion()"
      ],
      "metadata": {
        "id": "model_class"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FastAPI application\n",
        "app = FastAPI(\n",
        "    title=\"Text-to-Image AI (Colab)\",\n",
        "    description=\"Stable Diffusion text-to-image generation running in Google Colab\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "@app.post(\"/generate\", response_model=ImageResponse)\n",
        "async def generate_image_endpoint(request: ImageRequest):\n",
        "    \"\"\"Generate image from text prompt.\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Run generation in thread pool to avoid blocking\n",
        "        loop = asyncio.get_event_loop()\n",
        "        pil_image = await loop.run_in_executor(\n",
        "            None,\n",
        "            sd_model.generate_image,\n",
        "            request.prompt,\n",
        "            request.num_steps,\n",
        "            request.guidance_scale,\n",
        "            request.seed,\n",
        "        )\n",
        "\n",
        "        # Convert to base64\n",
        "        buffer = io.BytesIO()\n",
        "        pil_image.save(buffer, format=\"PNG\")\n",
        "        img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
        "\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        return ImageResponse(\n",
        "            image_base64=img_base64,\n",
        "            prompt=request.prompt,\n",
        "            generation_time=generation_time,\n",
        "            parameters={\n",
        "                \"num_steps\": request.num_steps,\n",
        "                \"guidance_scale\": request.guidance_scale,\n",
        "                \"seed\": request.seed\n",
        "            }\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Health check endpoint.\"\"\"\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"model_loaded\": sd_model.model is not None,\n",
        "        \"gpu_available\": gpu_available,\n",
        "        \"environment\": \"Google Colab\"\n",
        "    }\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Root endpoint with usage instructions.\"\"\"\n",
        "    return {\n",
        "        \"message\": \"Text-to-Image AI Service running in Google Colab\",\n",
        "        \"docs_url\": \"/docs\",\n",
        "        \"health_url\": \"/health\",\n",
        "        \"generate_url\": \"/generate\",\n",
        "        \"gpu_available\": gpu_available\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ FastAPI application created\")"
      ],
      "metadata": {
        "id": "fastapi_app"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Start the Server with Public URL"
      ],
      "metadata": {
        "id": "step5_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the server with ngrok tunnel\n",
        "def start_server():\n",
        "    \"\"\"Start the FastAPI server.\"\"\"\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ TEXT-TO-IMAGE AI SERVICE IS STARTING!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üåê Public URL: {public_url}\")\n",
        "print(f\"üìñ API Documentation: {public_url}/docs\")\n",
        "print(f\"‚ù§Ô∏è Health Check: {public_url}/health\")\n",
        "print(f\"üé® Generate Images: {public_url}/generate\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nüì± How to use:\")\n",
        "print(\"1. Click the API Documentation link above\")\n",
        "print(\"2. Try the /generate endpoint with a text prompt\")\n",
        "print(\"3. Or use curl/Python requests to call the API\")\n",
        "print(\"\\n‚ö†Ô∏è Note: Keep this cell running to maintain the service\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Start the server in a separate thread\n",
        "server_thread = threading.Thread(target=start_server)\n",
        "server_thread.daemon = True\n",
        "server_thread.start()\n",
        "\n",
        "print(\"\\n‚úÖ Server started! The service is now accessible via the public URL above.\")"
      ],
      "metadata": {
        "id": "start_server"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Test the API (Optional)"
      ],
      "metadata": {
        "id": "step6_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the API directly from the notebook\n",
        "import requests\n",
        "import json\n",
        "from IPython.display import Image as IPImage\n",
        "from io import BytesIO\n",
        "\n",
        "def test_generation(prompt, num_steps=25):\n",
        "    \"\"\"Test image generation directly.\"\"\"\n",
        "    print(f\"Testing generation with prompt: {prompt}\")\n",
        "\n",
        "    # Make API request\n",
        "    response = requests.post(\n",
        "        f\"{public_url}/generate\",\n",
        "        json={\n",
        "            \"prompt\": prompt,\n",
        "            \"num_steps\": num_steps,\n",
        "            \"guidance_scale\": 7.5\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        print(f\"‚úÖ Generation completed in {result['generation_time']:.1f} seconds\")\n",
        "\n",
        "        # Decode and display image\n",
        "        img_data = base64.b64decode(result['image_base64'])\n",
        "        return IPImage(img_data)\n",
        "    else:\n",
        "        print(f\"‚ùå Error: {response.status_code} - {response.text}\")\n",
        "        return None\n",
        "\n",
        "# Test with a simple prompt\n",
        "test_image = test_generation(\"a cute cat sitting in a garden\")\n",
        "if test_image:\n",
        "    display(test_image)"
      ],
      "metadata": {
        "id": "test_api"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Keep the Service Running"
      ],
      "metadata": {
        "id": "step7_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep the service running\n",
        "print(\"üîÑ Keeping the service running...\")\n",
        "print(f\"üåê Your service is available at: {public_url}\")\n",
        "print(\"\\nüí° Tips:\")\n",
        "print(\"- Keep this cell running to maintain the service\")\n",
        "print(\"- Use the /docs endpoint for interactive API testing\")\n",
        "print(\"- Lower num_steps (10-25) for faster generation\")\n",
        "print(\"- Higher guidance_scale (7.5-15) for more prompt adherence\")\n",
        "print(\"\\n‚ö†Ô∏è Note: Colab sessions timeout after ~12 hours of inactivity\")\n",
        "\n",
        "try:\n",
        "    # Keep the notebook alive\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "        print(f\"‚è∞ Service running... {time.strftime('%H:%M:%S')}\")\nexcept KeyboardInterrupt:\n",
        "    print(\"\\nüõë Service stopped by user\")\n",
        "    ngrok.disconnect(public_url)\n",
        "    print(\"‚úÖ Cleanup complete\")"
      ],
      "metadata": {
        "id": "keep_running"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example API Usage\n",
        "\n",
        "Once your service is running, you can use it from any application:\n",
        "\n",
        "### Python Example:\n",
        "```python\n",
        "import requests\n",
        "import base64\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Replace with your ngrok URL\n",
        "url = \"YOUR_NGROK_URL/generate\"\n",
        "\n",
        "response = requests.post(url, json={\n",
        "    \"prompt\": \"a beautiful sunset over mountains\",\n",
        "    \"num_steps\": 25,\n",
        "    \"guidance_scale\": 7.5\n",
        "})\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    img_data = base64.b64decode(result['image_base64'])\n",
        "    image = Image.open(BytesIO(img_data))\n",
        "    image.show()\n",
        "```\n",
        "\n",
        "### cURL Example:\n",
        "```bash\n",
        "curl -X POST \"YOUR_NGROK_URL/generate\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\n",
        "    \"prompt\": \"a futuristic city at night\",\n",
        "    \"num_steps\": 30,\n",
        "    \"guidance_scale\": 8.0\n",
        "  }'\n",
        "```"
      ],
      "metadata": {
        "id": "usage_examples"
      }
    }
  ]
}